{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchio as tio\n",
    "import torchvision.transforms as transforms\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all image tensors for synapses\n",
    "# These images are pre-padded and should be cut down to a size of 64^3 vx after augmentation transformations are performed\n",
    "imgpath = './synaptic_feature_vectors/input_images_unpadded_old/' # <- using old images just so we can get this code working while correct images are generated\n",
    "files = [f for f in listdir(imgpath) if isfile(join(imgpath,f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image resolution so that voxel spacing in mm can be assigned to\n",
    "# the images for each subject (translations for RandomAffine() are defined in mm)\n",
    "mm_per_nm = 1e-6\n",
    "xres_mmpvx = yres_mmpvx = 32*mm_per_nm\n",
    "zres_mmpvx = 40*mm_per_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate training and test sets (80% and 20% of samples, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PCT = 0.80\n",
    "train_set_size = int(np.ceil(TRAIN_PCT*len(files)))\n",
    "train_files = random.sample(files,k=train_set_size)\n",
    "test_files = list(set(files) - set(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_files)) # debugging\n",
    "# print(len(test_files))\n",
    "# print(len(train_files)/len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read (1) training and (2) test images and labels into subject lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_raw = pd.read_csv('./synaptic_feature_vectors/pyr_inh_syn_feature_vectors_old_1.csv',index_col=0)\n",
    "# Pre-processing step for current copy of label df: drop the 'cleft_vx_list' column. (It was mistakenly included.)\n",
    "# This column won't be present in future versions of this df.\n",
    "all_labels_raw.drop(columns=['cleft_vx_list'],inplace=True)\n",
    "# Drop mito size since that won't be predictable (is much larger than these small volumes)\n",
    "all_labels_raw.drop(columns=['mito_sizes_vx'],inplace=True)\n",
    "# Drop rows for which the number of mitochondria is nan\n",
    "all_labels_raw.drop(all_labels_raw[pd.isna(all_labels_raw['n_mitos'])].index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label definitions:\n",
    "\n",
    "0: excitatory cell\n",
    "\n",
    "1: inhibitory cell\n",
    "\n",
    "These classes apply to both the presynaptic cell and postsynaptic cell labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_labels_raw.head(1) # debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass: ONLY predict on the cell types. You can add the features back in later.\n",
    "all_labels = all_labels_raw[['syn_id','presyn_cell_type','postsyn_cell_type']]\n",
    "prect_int = [1 if q == 'e' else 0 for q in list(all_labels_raw['presyn_cell_type'])]\n",
    "postct_int = [1 if q == 'e' else 0 for q in list(all_labels_raw['postsyn_cell_type'])]\n",
    "all_labels = pd.DataFrame(data={'syn_id':all_labels_raw['syn_id'],'presyn_cell_type':prect_int,'postsyn_cell_type':postct_int})\n",
    "# all_labels = all_labels_raw[['syn_id','presyn_cell_subtype','postsyn_cell_subtype']]\n",
    "# print(np.unique(all_labels1['presyn_cell_subtype'])) # debugging\n",
    "# print(np.unique(all_labels1['postsyn_cell_subtype'])) # debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of outputs\n",
    "n_labels = len(all_labels.loc[all_labels.index[0]].values.tolist()) - 1 # subtract 1 to remove the syn_id key column\n",
    "print(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "        imgpath,\n",
    "        file_set,\n",
    "        scale_array=None,\n",
    "        transform=None,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        dtype=torch.float32):\n",
    "    \n",
    "    # Read in image tensors to a subject list\n",
    "    file_extension = '.npy'\n",
    "    synapse_images = []\n",
    "\n",
    "    for f in file_set[1:500]:\n",
    "        pathcurr = imgpath + f\n",
    "        synidcurr = f[:-len(file_extension)]\n",
    "        # Define labels according to one-hot construction\n",
    "        # Is this the best place to define labels in this way?\n",
    "        labelscurr = torch.tensor(\n",
    "            all_labels.loc[all_labels['syn_id'] == int(synidcurr)][['presyn_cell_type','postsyn_cell_type']].values[0, :],\n",
    "            device=device, dtype=torch.float32) # torch.long raised an error\n",
    "        tensorcurr = np.squeeze(np.load(pathcurr))\n",
    "\n",
    "        # Apply (hand-designed, for now) corrections to the tensor to standardize values\n",
    "        # onto the interval [0,1]\n",
    "        # also, clamp everything to 1.\n",
    "        tensorcurr = torch.tensor(tensorcurr / scale_array[:, None, None, None], device=device, dtype=dtype)\n",
    "        tensorcurr = torch.clamp(tensorcurr, min=0., max=1.)\n",
    "        \n",
    "        # Put tensor into a torchio scalar image object\n",
    "        imcurr = tio.ScalarImage(\n",
    "            tensor=tensorcurr,\n",
    "            spacing=(xres_mmpvx,yres_mmpvx,zres_mmpvx))\n",
    "\n",
    "        synimg_subject_curr = tio.Subject(\n",
    "            images=imcurr,\n",
    "            synid=synidcurr,\n",
    "            labels=labelscurr)\n",
    "\n",
    "        # Append to subject list (apparently subject lists do not pull images at instantiation; they only pull\n",
    "        # at the time when an operation is done)\n",
    "        synapse_images.append(synimg_subject_curr)\n",
    "        \n",
    "    # Build dataset\n",
    "    dataset = tio.data.SubjectsDataset(\n",
    "        subjects=synapse_images,\n",
    "        load_getitem=False,\n",
    "        transform=transform)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaling parameters for the channels in this dataset\n",
    "# n_mitos_max = np.max(list(all_labels_raw['n_mitos']))\n",
    "# scales = [255,1,1,1,n_mitos_max]\n",
    "# Scale mitochondria channel so all mitochondria have label 1\n",
    "scale_array = np.asarray([255,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(imgpath,train_files,scale_array=scale_array)#,transform=transform_std)\n",
    "test_dataset = generate_dataset(imgpath,test_files,scale_array=scale_array)#,transform=transform_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for augmentation transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the widths of each synapse image in mm, so that the translation bounds for RandomAffine() can be determined\n",
    "dxvx = dyvx = dzvx = 64\n",
    "dxmm = dxvx * xres_mmpvx\n",
    "dymm = dyvx * yres_mmpvx\n",
    "dzmm = dzvx * zres_mmpvx\n",
    "\n",
    "# Set parameter that *should* allow a crop to 32 px on a side to result in completely information-full images\n",
    "# post-transform\n",
    "transl_frac = 0.09\n",
    "rot_min_deg = 0\n",
    "rot_max_deg = 360\n",
    "\n",
    "# Set other affine parameters\n",
    "scale_min = 0.95\n",
    "scale_max = 1.05\n",
    "\n",
    "# Set finalized crop shape based on your calculations above\n",
    "final_shape = 32\n",
    "\n",
    "# Next, you need to check that 32 px on a side is sufficient to capture a cleft and some\n",
    "# of its pre- and postsynaptic partners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct transforms that will be eventually applied to training items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomFlip(). Allow flip about any axis\n",
    "randflip = tio.RandomFlip(axes = (0,1,2),flip_probability = 1.0) # flip across any axis\n",
    "\n",
    "# RandomAffine(). Thoughts about parameters:\n",
    "# scales: no scaling, because size is a potentially important parameter\n",
    "# degrees of rot.: could in priniciple be by any amount about any axis\n",
    "# scale and rotate about image center\n",
    "# shift in any dir by 16 px?\n",
    "randaff = tio.RandomAffine(degrees = (rot_min_deg,rot_max_deg), \\\n",
    "                           center = 'image', \\\n",
    "                           translation = (transl_frac*dxmm,transl_frac*dymm,transl_frac*dzmm) \\\n",
    "                          )\n",
    "\n",
    "# Intensity-based transforms\n",
    "# RandomNoise()\n",
    "# Gaussian noise. mu chosen on U(arg1,arg2) or U(-arg,+arg), default 0.\n",
    "# std chosen on U(arg1,arg2) or U(-arg,+arg), default (0,0.25)\n",
    "randnoise = tio.RandomNoise(mean=(0,0.5),std=(0,0.25))\n",
    "\n",
    "# RandomGamma()\n",
    "# Alters contrast by exponentiating elements. Argument is gamma = e^beta\n",
    "# beta is chosen on U(arg1,arg2) or U(-arg,+arg). default is (-0.3,0.3)\n",
    "randgamma = tio.RandomGamma()\n",
    "\n",
    "# Crop down to 32 px on a side\n",
    "crop = tio.CropOrPad(target_shape=final_shape) # chunks are initially padded to 64 px\n",
    "\n",
    "# Compose transforms\n",
    "# aug_transforms = tio.Compose([randflip,tio.OneOf({randaff:1.0},p=0.5),crop]) # code showing how to make prob(aff)=0.5\n",
    "aug_transforms = tio.Compose([randflip,randaff,randnoise,randgamma,crop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STRATEGY:\n",
    "\n",
    "Implement LeNet in 3D, 80% train/20% test split, predict only types of pre- and postsynaptic cells, keep track of loss per iteration for train set and loss per 100 iterations for test set (pull samples randomly from test set for that computation) using categorical cross entropy as loss, and keep track of accuracy in prediction per iteration for training set and per 100 iterations for test set)\n",
    "\n",
    "ALSO\n",
    "\n",
    "Prepare to discuss your progress and overall goals in a meeting with Antoni from the Broad Institute in the beginning of March if you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            device: torch.device = torch.device(\"cpu\"),\n",
    "            dtype: torch.dtype = torch.float32):\n",
    "        super(Net, self).__init__()        \n",
    "        # 5 input image channels per sample (32 x 32 x 32 vx)\n",
    "        # initial arg1 was 30 instead of 10\n",
    "        self.conv1 = nn.Conv3d(5,10,5) # Starting smaller just to see what will run (2 features per channel ish)\n",
    "        \n",
    "        # Second convolutional step\n",
    "        # initial arg0: 30. initial arg1: 80\n",
    "        self.conv2 = nn.Conv3d(10,20,5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20*5*5*5,120) # 20 channels at 5x5*5 px post-max pool step (again trying to start smaller)\n",
    "        self.fc2 = nn.Linear(120,84) # not sure why these intermediate steps are necessary\n",
    "        self.fc3 = nn.Linear(84,n_labels) # get it down to a size 14 vector to match the features you've measured?\n",
    "        \n",
    "        self.to(device)\n",
    "        self.type(dtype)\n",
    "        \n",
    "    # You define the forward() function and the backward() function is automatically defined using autograd\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2,2,2) window\n",
    "        x = F.max_pool3d(F.relu(self.conv1(x)), 2) # the maxpool is performed on the ReLU of the convolved input\n",
    "        # print(np.shape(x)) # debugging\n",
    "        \n",
    "        # Max pooling over a (2,2,2) window after performing convolution #2 on ^that result\n",
    "        x = F.max_pool3d(F.relu(self.conv2(x)), 2)\n",
    "        # print(np.shape(x)) # debugging\n",
    "        \n",
    "        # We're now flattening the whole set of features across all channels and we want to downsample?\n",
    "        # Or reshape?\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # print(np.shape(x)) # debugging\n",
    "        \n",
    "        # Turn maxpooled images into a fully-connected vector and apply relu to remove negative activations\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Don't perform ReLU on the final output because we want to see what the actual results are\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x)     :\n",
    "        size = x.size()[1:] # All dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START HERE: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data in mini-batches, augment each item in the structure appropriate for contrastive learning, and implement contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch loading object for subject dataset using PyTorch DataLoader object\n",
    "BATCH_SIZE = 2\n",
    "train_batch_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE)\n",
    "test_batch_loader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the net\n",
    "net = Net()\n",
    "# Initialize cross entropy loss criterion\n",
    "# (this one is classification-label-based loss)\n",
    "criterion = nn.BCELoss()\n",
    "# Initialize contrastive loss criterion\n",
    "# (this one is metric-learning-based loss)\n",
    "# TO DO: define or initialize if it already exists\n",
    "#contrast_criterion = []\n",
    "# Set learning rate\n",
    "LR = 0.001 # if this learning rate does not lead to settling with Adam, decrease to 10^-3\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set iteration frequency at which checks will occur\n",
    "N_ITERS_LOG_TRAIN = 100#2000\n",
    "N_ITERS_TEST_FRAC = 10#20\n",
    "N_ITERS_LOG_TEST = N_ITERS_LOG_TRAIN/N_ITERS_TEST_FRAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Set more reasonable parameters for number of epochs; add augmentation (and then contrastive augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# batchdata = next(iter(train_batch_loader))\n",
    "# bi = batchdata['images']['data']\n",
    "# bi = batchdata.imag\n",
    "# bic = torch.from_numpy(np.expand_dims(crop(np.squeeze(bi)),axis=0))\n",
    "# bic = torch.rand((5,5,32,32,32))\n",
    "# out = net(bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_apply(batch,transform,fshape=None):\n",
    "    # There has to be a built-in function for this, right?\n",
    "    if fshape == None:\n",
    "        fshape = np.shape(batch[-1])\n",
    "    batchfinal = torch.tensor(\n",
    "        np.zeros((np.shape(batch)[0],np.shape(batch)[1],fshape,fshape,fshape)),\n",
    "        device=torch.device(\"cpu\"),\n",
    "        dtype=torch.float32)\n",
    "    for bidx in range(len(batch)):\n",
    "        batchfinal[bidx] = transform(batch[bidx])\n",
    "        \n",
    "    return batchfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(t1,t2):\n",
    "    # @t1, @t2: input tensors\n",
    "    # returns cosine similarity by performing inner product of normalized tensors\n",
    "    t1 = torch.squeeze(t1)\n",
    "    t2 = torch.squeeze(t2)\n",
    "    norm = torch.multiply(torch.norm(t1),torch.norm(t2))\n",
    "    cos = torch.divide(torch.dot(t1,t2),norm)\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(pos,neg,temp=0.07):\n",
    "    # @pos: similarity function of the positive input\n",
    "    # @neg: similarity function of the negative input\n",
    "    # computes and returns contrastive log loss\n",
    "    alllogits = torch.cat((pos,neg))/temp\n",
    "    expinputs = torch.exp(alllogits)\n",
    "    loss = -torch.log(expinputs[0]/torch.sum(expinputs))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "tensor([[0.0116, 0.0769]], grad_fn=<AddmmBackward>) tensor([[0.0130, 0.0795]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.9999, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-5bd9ba3b6643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#         print(torch.shape(arg2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mlosscontrast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mptoutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mntoutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Sum losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossa\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlossp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlossn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlosscontrast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ccbe528e155b>\u001b[0m in \u001b[0;36mcontrastive_loss\u001b[0;34m(pos, neg, temp)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# @neg: similarity function of the negative input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# computes and returns contrastive log loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0malllogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mexpinputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malllogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpinputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(4): # loop over the dataset multiple times\n",
    "    print('epoch = {0}'.format(epoch))\n",
    "    \n",
    "    # Set up interactive loss plotting\n",
    "    plt.figure()\n",
    "    plt.xlabel('log iter')\n",
    "    plt.ylabel('loss')\n",
    "    iters_to_plot = []\n",
    "    rl_train_to_plot = []\n",
    "    rl_test_to_plot = []\n",
    "    running_loss_train = 0\n",
    "    running_loss_test = 0\n",
    "    \n",
    "    for i,data in enumerate(train_batch_loader,start=0):\n",
    "        testiter = iter(test_batch_loader)\n",
    "        batchinputsraw = data['images']['data']\n",
    "        batchlabels = data['labels']\n",
    "           \n",
    "        # Option 1: use a contrastive approach to augment the data and constrain\n",
    "        # the loss computation/weight updates\n",
    "        # Choose anchor and contrasting element at random from the minibatch\n",
    "        # (for transformation invariance)\n",
    "        ids = np.random.choice(np.shape(batchinputsraw)[0],2)\n",
    "        anchor = batchinputsraw[ids[0]]\n",
    "        negative = batchinputsraw[ids[1]]\n",
    "        # Get anchor and negative labels\n",
    "        alabel = torch.unsqueeze(batchlabels[ids[0]],0)\n",
    "        nlabel = torch.unsqueeze(batchlabels[ids[1]],0)\n",
    "        # Generate contrasting transforms\n",
    "        anchortrans = torch.unsqueeze(aug_transforms(anchor),0)\n",
    "        postrans = torch.unsqueeze(aug_transforms(anchor),0)\n",
    "        negtrans = torch.unsqueeze(aug_transforms(negative),0)\n",
    "        # Run all through the net\n",
    "        optimizer.zero_grad()\n",
    "        atoutput = net(anchortrans)\n",
    "        ptoutput = net(postrans)\n",
    "        ntoutput = net(negtrans)\n",
    "        # Compute loss function components:\n",
    "        # (1) Compute classification label log loss (cross-entropy log loss)\n",
    "        lossa = criterion(torch.sigmoid(atoutput),alabel)\n",
    "        lossp = criterion(torch.sigmoid(ptoutput),alabel)\n",
    "        lossn = criterion(torch.sigmoid(ntoutput),nlabel)\n",
    "        # (2) Compute contrastive metric learning log loss\n",
    "        arg1 = cosine_sim(atoutput,ptoutput)\n",
    "        print(atoutput,ptoutput)\n",
    "        print(arg1)\n",
    "        arg2 = cosine_sim(atoutput,ntoutput)\n",
    "#         print(torch.shape(arg2))\n",
    "    \n",
    "        losscontrast = contrastive_loss(cosine_sim(atoutput,ptoutput),cosine_sim(atoutput,ntoutput))\n",
    "        # Sum losses\n",
    "        loss = lossa + lossp + lossn + losscontrast\n",
    "        \n",
    "        # Option 2: Augment the whole minibatch and update that way\n",
    "        # Augment minibatch\n",
    "        # batchinputs = batch_apply(batchinputsraw,aug_transforms,fshape=final_shape)\n",
    "        # When augmentation is not used, crop images to 32 px on a side\n",
    "        # batchinputs = batch_apply(batchinputsraw,crop,fshape=final_shape)\n",
    "        # Zero parameter gradient buffers\n",
    "        # optimizer.zero_grad()\n",
    "        # Forward + backward + optimize\n",
    "        # outputs = net(batchinputs)\n",
    "        # loss = criterion(torch.sigmoid(outputs),batchlabels)\n",
    "\n",
    "        # Use loss to perform gradient-based weight updates\n",
    "        loss.backward() # see how much the loss changed with differences in weights\n",
    "        optimizer.step() # update the weights in the network\n",
    "        running_loss_train += loss.item()\n",
    "        \n",
    "        # Compute test set loss at regular intervals\n",
    "        if i % N_ITERS_LOG_TEST == (N_ITERS_LOG_TEST-1):\n",
    "            tbc = testiter.next()\n",
    "            testinputs = tbc['images']['data']\n",
    "            testinputscropped = batch_apply(testinputs,crop,fshape=final_shape)\n",
    "            testlabels = tbc['labels']\n",
    "            testoutputs = net(testinputscropped)\n",
    "            losstest = criterion(torch.sigmoid(testoutputs),testlabels)\n",
    "            running_loss_test += losstest.item()\n",
    "        \n",
    "        # Plot average train and test losses\n",
    "        if i % N_ITERS_LOG_TRAIN == (N_ITERS_LOG_TRAIN-1): \n",
    "            print('checking test and train loss at iter {0}'.format(i))\n",
    "            rl_train_avg = running_loss_train/N_ITERS_LOG_TRAIN\n",
    "            rl_test_avg = running_loss_test/N_ITERS_LOG_TEST\n",
    "            iters_to_plot.append(i)\n",
    "            rl_train_to_plot.append(rl_train_avg)\n",
    "            rl_test_to_plot.append(rl_test_avg)\n",
    "            \n",
    "            # Plot average running training and test losses\n",
    "            plt.plot(iters_to_plot,rl_train_to_plot,'-.',color='tab:blue')\n",
    "            plt.plot(iters_to_plot,rl_train_to_plot,'.',color='tab:blue',markersize=10,label='training loss')\n",
    "            plt.plot(iters_to_plot,rl_test_to_plot,'-.',color='tab:orange')\n",
    "            plt.plot(iters_to_plot,rl_test_to_plot,'.',color='tab:orange',markersize=10,label='test loss')\n",
    "            plt.legend(loc='best')\n",
    "            plt.show()\n",
    "            \n",
    "            # Reset running losses for next set of 2000 iterations\n",
    "            running_loss_train = 0.0\n",
    "            running_loss_test = 0.0\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normcheck = torch.divide(torch.sigmoid(atoutput),torch.norm(torch.sigmoid(atoutput)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(normcheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refs:\n",
    "# https://torchio.readthedocs.io/transforms/augmentation.html#spatial\n",
    "# https://torchio.readthedocs.io/data/dataset.html\n",
    "# https://torchio.readthedocs.io/quickstart.html\n",
    "\n",
    "\n",
    "# Information about composing transformations:\n",
    "# https://torchio.readthedocs.io/transforms/transforms.html#composability\n",
    "\n",
    "# Information about patch-based pipelines:\n",
    "# https://torchio.readthedocs.io/data/patch_based.html\n",
    "\n",
    "# PyTorch DataLoader:\n",
    "# https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "# Example with a CNN (LeNet):\n",
    "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "# Info about backprop\n",
    "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#backprop\n",
    "\n",
    "# Another helpful derivation of backpropagation and neural network training:\n",
    "\n",
    "# Log loss:\n",
    "# https://jbencook.com/cross-entropy-loss-in-pytorch/\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
